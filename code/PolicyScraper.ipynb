{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "''' Scrapes the password policies from Alexa's top websites '''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "__author__ = \"Aditya Godambe\"\n",
    "__email__ = \"agodambe@cs.stonybrook.edu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs from Alexa top websites for USA\n",
    "alexa_urls = []\n",
    "with open(\"sites.csv\", 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        alexa_urls.append(line.rstrip())\n",
    "\n",
    "\n",
    "# List of urls not to be scraped\n",
    "blacklist = []\n",
    "with open(\"blacklist.csv\", 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        blacklist.append(line.rstrip())\n",
    "\n",
    "\n",
    "# Bag of words for different policies\n",
    "policies = []\n",
    "with open(\"policies.csv\",'r') as file:\n",
    "    for line in file.readlines():\n",
    "        policies.append(line.rstrip())\n",
    "\n",
    "\n",
    "\n",
    "# Bag of words for Sign Up action\n",
    "sign_up_list = []\n",
    "with open(\"signup.csv\",'r') as file:\n",
    "    for line in file.readlines():\n",
    "        sign_up_list.append(line.rstrip())\n",
    "\n",
    "\n",
    "# Bag of words for Sign in action\n",
    "sign_in_list = []\n",
    "with open(\"signin.csv\",'r') as file:\n",
    "    for line in file.readlines():\n",
    "        sign_in_list.append(line.rstrip())\n",
    "        \n",
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildUrl(protocol, target, old_target):\n",
    "    '''\n",
    "        This function builds a complete url based on available link\n",
    "        Args:\n",
    "            protocol: The protocol used by the website, set default to https://wwww.\n",
    "            target: The portion of the new url that needs to be completed\n",
    "            old_target: The url of the current page, typically, the sign in page\n",
    "        Returns:\n",
    "            complete url to be requested (str)\n",
    "    '''\n",
    "    data = requests.request(\"GET\", old_target)\n",
    "    url = data.url\n",
    "    netlocal = urlparse(url).netloc\n",
    "    \n",
    "    #print(netlocal)\n",
    "    #print(protocol)\n",
    "    \n",
    "    return protocol + netlocal + target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPolicy(url, soup, policy):\n",
    "    '''\n",
    "        This function extracts the policy present on the page\n",
    "        Args:\n",
    "            soup: BeautifulSoup object for the current page\n",
    "            policy: The policy keywords found on the page\n",
    "        Returns:\n",
    "            Currently, prints the original URL and the password policy\n",
    "    '''\n",
    "    \n",
    "    final_policy = \"\"\n",
    "    \n",
    "    policy_str = soup.find(string = re.compile(policy))\n",
    "    index = policy_str.find(policy)\n",
    "    \n",
    "    while index < len(policy_str):\n",
    "        if policy_str[index] == \".\" or policy_str[index] == \")\":\n",
    "            break\n",
    "        final_policy = final_policy + policy_str[index]\n",
    "        index = index+1\n",
    "    \n",
    "    #print(url)\n",
    "    #print(final_policy)\n",
    "    data[protocol+url] = final_policy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def userAccountPresent(soup):\n",
    "    '''\n",
    "        This function returns true if the site has an option to create a user account\n",
    "        Args: \n",
    "            soup: BeautifulSoup object for the current page\n",
    "        Returns:\n",
    "            True, if there is an option to create account\n",
    "            False, otherwise\n",
    "    '''\n",
    "    #sign_in_present = False\n",
    "    #sign_up_present = False\n",
    "    \n",
    "    # Check if Sign in option is present\n",
    "    for sign_in in sign_in_list:\n",
    "        if len(soup.find_all(\"a\")) != 0:\n",
    "            for link in soup.find_all(\"a\"):\n",
    "                if len(link) != 0:\n",
    "                    if sign_in in link: #or sign_in in link.get(\"href\"):\n",
    "                        return True\n",
    "    \n",
    "    for sign_up in sign_up_list:\n",
    "        if len(soup.find_all(\"a\")) != 0:\n",
    "            for link in soup.find_all(\"a\"):\n",
    "                if len(link) != 0:\n",
    "                    if sign_up in link:# or sign_up in link.get(\"href\"):\n",
    "                        return True\n",
    "    \n",
    "    return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkPolicies(soup):\n",
    "    '''\n",
    "        This funtion checks if the page contains the policies\n",
    "        Args:\n",
    "            soup: BeautifulSoup object for the current page\n",
    "        Returns:\n",
    "            True, if policy found\n",
    "            False, otherwise\n",
    "    '''\n",
    "    policy_found = False\n",
    "    for policy in policies:\n",
    "        if len(soup.find_all(string = re.compile(policy))) != 0:\n",
    "            # Policy exists on this page\n",
    "            policy_found = True\n",
    "            extractPolicy(url, soup, policy)\n",
    "    \n",
    "    return policy_found\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check sign up in homepage also eg: twitch.tv\n",
    "# TODO: Fix stackoverflow.com issue: policies not being scraped\n",
    "# TODO: connection error issue with some websites: too many requests\n",
    "\n",
    "\n",
    "# Traverse through the list of urls, skip blacklisted ones\n",
    "\n",
    "protocol = \"https://www.\"\n",
    "site_number = 0\n",
    "\n",
    "for url in alexa_urls:\n",
    "    url = url.lower()\n",
    "    site_number = site_number+1\n",
    "    #print(\"\\n\" + str(site_number) + \". \" + url)\n",
    "    \n",
    "    blacklisted = False\n",
    "    policy_found = False\n",
    "    sign_up_found = False\n",
    "    sign_in_found = False\n",
    "    \n",
    "    \n",
    "    for blacklist_keyword in blacklist:\n",
    "        if blacklist_keyword in url:\n",
    "            blacklisted = True\n",
    "            break\n",
    "    if blacklisted:\n",
    "        #print(\"Scraping not allowed\")\n",
    "        data[protocol+url] = \"Scraping not allowed\"\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    # For urls that are not blacklisted, perform scraping\n",
    "    r = requests.get(protocol + url)\n",
    "    c = r.content\n",
    "    \n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    \n",
    "    # Check if this site requires a user account\n",
    "    if not userAccountPresent(soup):\n",
    "        continue\n",
    "    \n",
    "    # Check if policy exists on the home page\n",
    "    policy_found = checkPolicies(soup)\n",
    "    if policy_found:\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    # Check for Sign In keywords\n",
    "    target = \"\"\n",
    "    for sign_in in sign_in_list:\n",
    "        for link in soup.find_all(\"a\"):\n",
    "            if sign_in in link:# or sign_in in link.get(\"href\"):\n",
    "                target = link.get(\"href\")\n",
    "                sign_in_found = True\n",
    "                break\n",
    "        if sign_in_found:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    if target[0] == \"/\":\n",
    "        # Get complete url from the relative url\n",
    "        target = buildUrl(protocol, target, protocol+url)\n",
    "    \n",
    "    # Go to the target url and check for policy\n",
    "    try:\n",
    "        r = requests.get(target, timeout=5)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        r.status_code = \"Connection refused\"\n",
    "        \n",
    "    c = r.content\n",
    "    \n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    \n",
    "    # Check if policy exists on the Sign in page\n",
    "    policy_found = checkPolicies(soup)  \n",
    "    if policy_found:\n",
    "        continue\n",
    "        \n",
    "    # Policy not found in the Sign in page; Check for sign up keywords in the sign in page\n",
    "    old_target = target\n",
    "    for sign_up in sign_up_list:\n",
    "        for link in soup.find_all(\"a\"):\n",
    "            if sign_up in link or sign_up in link.get(\"href\"):\n",
    "                target = link.get(\"href\")\n",
    "                sign_up_found = True\n",
    "                break\n",
    "        if sign_up_found:\n",
    "            break\n",
    "    \n",
    "    new_url = \"\"\n",
    "    if target[0] == \"/\":\n",
    "        # Build a url for the complete sign up page\n",
    "        new_url = buildUrl(protocol, target, old_target)\n",
    "    else:\n",
    "        new_url = target\n",
    "    \n",
    "    # Go to the new url and check for policy\n",
    "    try:\n",
    "        r = requests.get(new_url)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        r.status_code = \"Connection refused\"\n",
    "    \n",
    "    \n",
    "    c = r.content\n",
    "    \n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    \n",
    "    # Check if policy exists on the Sign up page\n",
    "    policy_found = checkPolicies(soup)    \n",
    "    if not policy_found:\n",
    "        #print(\"Policy not found\")\n",
    "        data[protocol+url] = \"Policy not found\"\n",
    "        continue\n",
    "\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
