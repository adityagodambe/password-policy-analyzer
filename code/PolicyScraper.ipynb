{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "''' Scrapes the password policies from Alexa's top websites '''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "__author__ = \"Aditya Godambe\"\n",
    "__email__ = \"agodambe@cs.stonybrook.edu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs from Alexa top websites for USA\n",
    "alexa_urls = []\n",
    "with open(\"sites.csv\", 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        alexa_urls.append(line.rstrip())\n",
    "\n",
    "\n",
    "# List of urls not to be scraped\n",
    "blacklist = []\n",
    "with open(\"blacklist.csv\", 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        blacklist.append(line.rstrip())\n",
    "\n",
    "\n",
    "# Bag of words for different policies\n",
    "policies = []\n",
    "with open(\"policies.csv\",'r') as file:\n",
    "    for line in file.readlines():\n",
    "        policies.append(line.rstrip())\n",
    "\n",
    "\n",
    "\n",
    "# Bag of words for Sign Up action\n",
    "sign_up_list = []\n",
    "with open(\"signup.csv\",'r') as file:\n",
    "    for line in file.readlines():\n",
    "        sign_up_list.append(line.rstrip())\n",
    "\n",
    "\n",
    "# Bag of words for Sign in action\n",
    "sign_in_list = []\n",
    "with open(\"signin.csv\",'r') as file:\n",
    "    for line in file.readlines():\n",
    "        sign_in_list.append(line.rstrip())\n",
    "        \n",
    "# # Proxy list\n",
    "# proxies = {\"http\": \"http://10.10.1.10:3128\",\n",
    "#            \"https\": \"http://10.10.1.10:1080\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildUrl(protocol, target, old_target):\n",
    "    '''\n",
    "        This function builds a complete url based on available link\n",
    "        Args:\n",
    "            protocol: The protocol used by the website, set default to https://wwww.\n",
    "            target: The portion of the new url that needs to be completed\n",
    "            old_target: The url of the current page, typically, the sign in page\n",
    "        Returns:\n",
    "            complete url to be requested (str)\n",
    "    '''\n",
    "    data = requests.request(\"GET\", old_target)\n",
    "    url = data.url\n",
    "    netlocal = urlparse(url).netloc\n",
    "    \n",
    "    #print(netlocal)\n",
    "    #print(protocol)\n",
    "    \n",
    "    return protocol + netlocal + target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPolicy(url, soup, policy):\n",
    "    '''\n",
    "        This function extracts the policy present on the page\n",
    "        Args:\n",
    "            soup: BeautifulSoup object for the current page\n",
    "            policy: The policy keywords found on the page\n",
    "        Returns:\n",
    "            Currently, prints the original URL and the password policy\n",
    "    '''\n",
    "    \n",
    "    final_policy = \"\"\n",
    "    \n",
    "    policy_str = soup.find(string = re.compile(policy))\n",
    "    index = policy_str.find(policy)\n",
    "    \n",
    "    while index < len(policy_str):\n",
    "        if policy_str[index] == \".\" or policy_str[index] == \")\":\n",
    "            break\n",
    "        final_policy = final_policy + policy_str[index]\n",
    "        index = index+1\n",
    "    \n",
    "    #print(url)\n",
    "    print(final_policy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def userAccountPresent(soup):\n",
    "    '''\n",
    "        This function returns true if the site has an option to create a user account\n",
    "        Args: \n",
    "            soup: BeautifulSoup object for the current page\n",
    "        Returns:\n",
    "            True, if there is an option to create account\n",
    "            False, otherwise\n",
    "    '''\n",
    "    #sign_in_present = False\n",
    "    #sign_up_present = False\n",
    "    \n",
    "    # Check if Sign in option is present\n",
    "    for sign_in in sign_in_list:\n",
    "        if len(soup.find_all(\"a\")) != 0:\n",
    "            for link in soup.find_all(\"a\"):\n",
    "                if len(link) != 0:\n",
    "                    if sign_in in link: #or sign_in in link.get(\"href\"):\n",
    "                        return True\n",
    "    \n",
    "    for sign_up in sign_up_list:\n",
    "        if len(soup.find_all(\"a\")) != 0:\n",
    "            for link in soup.find_all(\"a\"):\n",
    "                if len(link) != 0:\n",
    "                    if sign_up in link:# or sign_up in link.get(\"href\"):\n",
    "                        return True\n",
    "    \n",
    "    return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkPolicies(soup):\n",
    "    '''\n",
    "        This funtion checks if the page contains the policies\n",
    "        Args:\n",
    "            soup: BeautifulSoup object for the current page\n",
    "        Returns:\n",
    "            True, if policy found\n",
    "            False, otherwise\n",
    "    '''\n",
    "    policy_found = False\n",
    "    for policy in policies:\n",
    "        if len(soup.find_all(string = re.compile(policy))) != 0:\n",
    "            # Policy exists on this page\n",
    "            policy_found = True\n",
    "            extractPolicy(url, soup, policy)\n",
    "    \n",
    "    return policy_found\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. google.com\n",
      "Scraping not allowed\n",
      "\n",
      "2. youtube.com\n",
      "Scraping not allowed\n",
      "\n",
      "3. facebook.com\n",
      "Enter a combination of at least six numbers, letters and punctuation marks (like ! and &amp;\n",
      "make sure you understand how Facebook Payments International collects personal information, why data is collected and how it's used and stored\n",
      "at least six numbers, letters and punctuation marks (like ! and &amp;\n",
      "letters and punctuation marks (like ! and &amp;\n",
      "numbers, letters and punctuation marks (like ! and &amp;\n",
      "\n",
      "4. amazon.com\n",
      "Scraping not allowed\n",
      "\n",
      "5. wikipedia.org\n",
      "\n",
      "6. reddit.com\n",
      "\n",
      "7. yahoo.com\n",
      "Policy not found\n",
      "\n",
      "8. twitter.com\n",
      "Scraping not allowed\n",
      "\n",
      "9. linkedin.com\n",
      "6 or more characters\n",
      "\n",
      "10. instagram.com\n",
      "\n",
      "11. ebay.com\n",
      "numbers\",\"capthaSecurityMsg\":\"For added security, please enter the verification code below\",\"captchaBrowserError\":\"Your browser doesn't support to play this audio\n",
      "\n",
      "12. netflix.com\n",
      "Scraping not allowed\n",
      "\n",
      "13. espn.com\n",
      "\n",
      "14. twitch.tv\n",
      "\n",
      "15. microsoftonline.com\n",
      "Scraping not allowed\n",
      "\n",
      "16. office.com\n",
      "\n",
      "17. pornhub.com\n",
      "at least 3 characters long\",\"errorShortTitle\":\"You must enter a title at least 3 characters long\",\"errorNoVideo\":\"Video doesn't exist\",\"errorCouldNotCreate\":\"playlist could not be created\",\"errorCantCreate\":\"Playlist can't be created\",\"errorTitleForbiddenWords\":\"Your playlist title contains forbidden words\",\"errorDescriptionToManyUrls\":\"Your playlist description has too many links\",\"successNewPlaylist\":\"Playlist Created\",\"successVideoAdded\":\"Video has been added to your new playlist\",\"createPL\":\"Create a new Playlist\",\"favoritePL\":\"Favorite this Playlist\",\"alreadyInFavorite\":\"Already in Favorites\",\"warningMaxVideos\":\"You have reached the maximum limit of videos inside a playlist\",\"urlNewPL\":\"\\/playlist_json\\/create\",\"urlFavoritePL\":\"\\/playlist_json\\/favourite\",\"urlRemoveFavoritePL\":\"\\/playlist\\/remove_favourite\",\"urladdToPL\":\"\\/playlist\\/add\"};\n",
      "\n",
      "\n",
      "18. imgur.com\n",
      "\n",
      "19. live.com\n",
      "\n",
      "20. craigslist.org\n",
      "Policy not found\n",
      "\n",
      "21. cnn.com\n",
      "\n",
      "22. t.co\n",
      "\n",
      "23. paypal.com\n",
      "at least {0} characters\",\n",
      "        \"MESSAGE_RANGELENGTH\": \"Please enter a value between {0} and {1} characters long\",\n",
      "        \"MESSAGE_RANGE\": \"Please enter a value between {0} and {1}\",\n",
      "        \"MESSAGE_MAX\": \"Please enter a value less than or equal to {0}\",\n",
      "        \"MESSAGE_MIN\": \"Please enter a value greater than or equal to {0}\",\n",
      "        \"MESSAGE_BADPHONE\": \"We can't send money to that phone number\n",
      "a number\",\n",
      "        \"MESSAGE_DIGITS\": \"Please enter only digits\",\n",
      "        \"MESSAGE_CREDITCARD\": \"Please enter a valid credit card number\",\n",
      "        \"MESSAGE_EQUALTO\": \"Your passwords don't match\n",
      "\n",
      "24. chase.com\n",
      "make sure JavaScript is turned on\n",
      "\n",
      "25. bing.com\n",
      "\n",
      "26. nytimes.com\n",
      "letters\n",
      "\n",
      "27. pinterest.com\n",
      "\n",
      "28. tumblr.com\n",
      "a number!\" : 'Please enter a number!',\n",
      "            \"Please enter your age as a number only!\" : 'Please enter your age as a number only!',\n",
      "\n",
      "            \"One more thing &ndash; please accept our Terms!\" : 'One more thing \\x26ndash; please accept our Terms!',\n",
      "            \"You must accept Tumblr's terms before proceeding\n",
      "\n",
      "29. imdb.com\n",
      "\n",
      "30. wikia.com\n",
      "\n",
      "31. instructure.com\n",
      "\n",
      "32. livejasmin.com\n",
      "\n",
      "33. apple.com\n",
      "\n",
      "34. microsoft.com\n",
      "Scraping not allowed\n",
      "\n",
      "35. salesforce.com\n",
      "\n",
      "36. github.com\n",
      "Make sure it's more than 15 characters, or at least 8 characters, including a number, and a lowercase letter\n",
      "at least 8 characters, including a number, and a lowercase letter\n",
      "a number, and a lowercase letter\n",
      "\n",
      "37. stackoverflow.com\n",
      "numbers in the sequence\n",
      "\n",
      "38. force.com\n",
      "\n",
      "39. walmart.com\n",
      "\n",
      "40. yelp.com\n",
      "Make sure you click \n",
      "make sure \n",
      "\n",
      "41. hulu.com\n",
      "Policy not found\n",
      "\n",
      "42. dropbox.com\n",
      "\n",
      "43. xvideos.com\n",
      "Scraping not allowed\n",
      "\n",
      "44. bankofamerica.com\n",
      "\n",
      "45. zillow.com\n",
      "\n",
      "46. wellsfargo.com\n",
      "\n",
      "47. quora.com\n",
      "\n",
      "48. blogspot.com\n",
      "\n",
      "49. amazonaws.com\n",
      "Scraping not allowed\n",
      "\n",
      "50. breitbart.com\n"
     ]
    }
   ],
   "source": [
    "# TODO: Check sign up in homepage also eg: twitch.tv\n",
    "# TODO: Fix stackoverflow.com issue: policies not being scraped\n",
    "# TODO: connection error issue with xvideos.com\n",
    "\n",
    "\n",
    "# Traverse through the list of urls, skip blacklisted ones\n",
    "\n",
    "protocol = \"https://www.\"\n",
    "site_number = 0\n",
    "\n",
    "for url in alexa_urls:\n",
    "    url = url.lower()\n",
    "    site_number = site_number+1\n",
    "    print(\"\\n\" + str(site_number) + \". \" + url)\n",
    "    \n",
    "    blacklisted = False\n",
    "    policy_found = False\n",
    "    sign_up_found = False\n",
    "    sign_in_found = False\n",
    "    \n",
    "    \n",
    "    for blacklist_keyword in blacklist:\n",
    "        if blacklist_keyword in url:\n",
    "            blacklisted = True\n",
    "            break\n",
    "    if blacklisted:\n",
    "        print(\"Scraping not allowed\")\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    # For urls that are not blacklisted, perform scraping\n",
    "    r = requests.get(protocol + url)\n",
    "    c = r.content\n",
    "    \n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    \n",
    "    # Check if this site requires a user account\n",
    "    if not userAccountPresent(soup):\n",
    "        continue\n",
    "    \n",
    "    # Check if policy exists on the home page\n",
    "    policy_found = checkPolicies(soup)\n",
    "    if policy_found:\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    # Check for Sign In keywords\n",
    "    target = \"\"\n",
    "    for sign_in in sign_in_list:\n",
    "        for link in soup.find_all(\"a\"):\n",
    "            if sign_in in link:# or sign_in in link.get(\"href\"):\n",
    "                target = link.get(\"href\")\n",
    "                sign_in_found = True\n",
    "                break\n",
    "        if sign_in_found:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    if target[0] == \"/\":\n",
    "        # Get complete url from the relative url\n",
    "        target = buildUrl(protocol, target, protocol+url)\n",
    "    \n",
    "    # Go to the target url and check for policy\n",
    "    try:\n",
    "        r = requests.get(target, timeout=5)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        r.status_code = \"Connection refused\"\n",
    "        \n",
    "    c = r.content\n",
    "    \n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    \n",
    "    # Check if policy exists on the Sign in page\n",
    "    policy_found = checkPolicies(soup)  \n",
    "    if policy_found:\n",
    "        continue\n",
    "        \n",
    "    # Policy not found in the Sign in page; Check for sign up keywords in the sign in page\n",
    "    old_target = target\n",
    "    for sign_up in sign_up_list:\n",
    "        for link in soup.find_all(\"a\"):\n",
    "            if sign_up in link or sign_up in link.get(\"href\"):\n",
    "                target = link.get(\"href\")\n",
    "                sign_up_found = True\n",
    "                break\n",
    "        if sign_up_found:\n",
    "            break\n",
    "    \n",
    "    new_url = \"\"\n",
    "    if target[0] == \"/\":\n",
    "        # Build a url for the complete sign up page\n",
    "        new_url = buildUrl(protocol, target, old_target)\n",
    "    else:\n",
    "        new_url = target\n",
    "    \n",
    "    # Go to the new url and check for policy\n",
    "    try:\n",
    "        r = requests.get(new_url)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        r.status_code = \"Connection refused\"\n",
    "    \n",
    "    \n",
    "    c = r.content\n",
    "    \n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    \n",
    "    # Check if policy exists on the Sign up page\n",
    "    policy_found = checkPolicies(soup)    \n",
    "    if not policy_found:\n",
    "        print(\"Policy not found\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
