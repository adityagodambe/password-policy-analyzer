{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a combination of at least six numbers, letters and punctuation marks (like ! and &amp;)\n"
     ]
    }
   ],
   "source": [
    "# Solution for www.facebook.com\n",
    "# ------------------WORKS--------------------------- #\n",
    "\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get(\"https://www.facebook.com/\")\n",
    "c = r.content\n",
    "\n",
    "soup = BeautifulSoup(c,\"html.parser\")\n",
    "text_content = str(c)\n",
    "\n",
    "\n",
    "policy_facebook = \"Enter a combination of\"\n",
    "\n",
    "# find the string policy_facebook in the site content\n",
    "nav_str = soup.find(string = re.compile(policy_facebook))\n",
    "#print(policy_facebook in nav_str)\n",
    "\n",
    "index = nav_str.find(policy_facebook)\n",
    "#final policy to be extracted\n",
    "res_policy = \"\"\n",
    "\n",
    "while nav_str[index] != \".\":\n",
    "    res_policy = res_policy + nav_str[index]\n",
    "    index = index+1\n",
    "    \n",
    "print(res_policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.getpostman.com/signup?redirect=web\n",
      "identity.getpostman.com\n",
      "Passwords should be at least 7 characters long. Try using uncommon words or inside jokes, non-standard uppercasing, creative spelllling, and non-obvious numbers and symbols.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test for extracting the sign up page\n",
    "# First look for sign in\n",
    "# Then look for Create account\n",
    "\n",
    "# ------------------WORKS--------------------------- #\n",
    "\n",
    "signin = \"signup\"\n",
    "signin2 = \"signup\"\n",
    "signup = \"Create Account\"\n",
    "orig_site = \"https://www.getpostman.com\"\n",
    "postman_policy = \"Passwords should be\"\n",
    "\n",
    "postman_r = requests.get(\"https://www.getpostman.com\")\n",
    "postman_c = postman_r.content\n",
    "\n",
    "soup = BeautifulSoup(postman_c, \"html.parser\")\n",
    "postman_text = str(postman_c)\n",
    "\n",
    "#print(signin in postman_text)\n",
    "\n",
    "for link in soup.find_all(\"a\"):\n",
    "    #print(link.get(\"href\"))\n",
    "    \n",
    "    target = link.get(\"href\")\n",
    "    if signin in target or signup in target or signin2 in target:\n",
    "        #print(target)\n",
    "        break\n",
    "    \n",
    "    \n",
    "print(target)\n",
    "# capture the netloc of this url\n",
    "\n",
    "\n",
    "# obtain the url of the visited page\n",
    "# trying to get \"www.identity.getpostman.com\"\n",
    "data = requests.request(\"GET\", target)\n",
    "url = data.url\n",
    "#print(url)\n",
    "\n",
    "netloc = urlparse(url).netloc\n",
    "print(netloc)\n",
    "\n",
    "\n",
    "# visit this new page\n",
    "postman_r = requests.get(target)\n",
    "postman_c = postman_r.content\n",
    "\n",
    "soup = BeautifulSoup(postman_c, \"html.parser\")\n",
    "postman_text = str(postman_c)\n",
    "\n",
    "#print(postman_text)\n",
    "\n",
    "for link in soup.find_all(\"a\"):\n",
    "    #print(link.get(\"href\"))\n",
    "    \n",
    "    target = link.get(\"href\")\n",
    "    if signin in target or signup in target or signin2 in target:\n",
    "        #print(target[0])\n",
    "        if target[0] == \"/\":\n",
    "            target = \"https://\" + netloc + target\n",
    "            #print(target)\n",
    "        break\n",
    "    \n",
    "    \n",
    "#print(target)\n",
    "\n",
    "\n",
    "# Parse URL using urllib\n",
    "# target is our final site to look for policies\n",
    "# scrape target to get the policies\n",
    "\n",
    "# visit this new page\n",
    "postman_r = requests.get(target)\n",
    "postman_c = postman_r.content\n",
    "\n",
    "soup = BeautifulSoup(postman_c, \"html.parser\")\n",
    "postman_text = str(postman_c)\n",
    "\n",
    "#print(postman_text)\n",
    "\n",
    "#desc = soup.find_all(\"div\", {\"class\":\"strength-description\"})\n",
    "#print(type(desc))\n",
    "\n",
    "# Find the policy string in the page\n",
    "nav_str = soup.find(string = re.compile(postman_policy))\n",
    "print(nav_str)\n",
    "\n",
    "index = nav_str.find(policy_github)\n",
    "#final policy to be extracted\n",
    "res_policy = \"\"\n",
    "\n",
    "while nav_str[index] != \".\":\n",
    "    res_policy = res_policy + nav_str[index]\n",
    "    index = index+1\n",
    "    \n",
    "print(res_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure it's more than 15 characters, or at least 8 characters, including a number, and a lowercase letter.\n",
      "Make sure it's more than 15 characters, or at least 8 characters, including a number, and a lowercase letter\n"
     ]
    }
   ],
   "source": [
    "# www.github.com\n",
    "# ------------------WORKS--------------------------- #\n",
    "\n",
    "\n",
    "r = requests.get(\"https://www.github.com\")\n",
    "c = r.content\n",
    "\n",
    "soup = BeautifulSoup(c,\"html.parser\")\n",
    "text_content = str(c)\n",
    "\n",
    "\n",
    "#policy_facebook = \"Enter a combination of\"\n",
    "policy_github = \"Make sure\"\n",
    "\n",
    "#print(policy_github in text_content)\n",
    "\n",
    "# find the string policy_github in the site content\n",
    "nav_str = soup.find(string = re.compile(policy_github))\n",
    "print(nav_str)\n",
    "#print(policy_facebook in nav_str)\n",
    "\n",
    "index = nav_str.find(policy_github)\n",
    "#final policy to be extracted\n",
    "res_policy = \"\"\n",
    "\n",
    "while nav_str[index] != \".\":\n",
    "    res_policy = res_policy + nav_str[index]\n",
    "    index = index+1\n",
    "    \n",
    "print(res_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://mail.google.com/mail/?tab=wm\n",
      "https://accounts.google.com/ServiceLogin?service=mail&passive=true&rm=false&continue=https://mail.google.com/mail/?tab%3Dwm&scc=1&ltmpl=default&ltmplcache=2&emr=1&osid=1\n",
      "<a class=\"need-help\" href=\"https://accounts.google.com/signin/usernamerecovery?continue=https%3A%2F%2Fmail.google.com%2Fmail%2F%3Ftab%3Dwm&amp;service=mail&amp;scc=1&amp;rm=false&amp;osid=1&amp;hl=en\">\n",
      "  Find my account\n",
      "  </a>\n",
      "False\n",
      "<a href=\"https://accounts.google.com/AccountChooser?continue=https%3A%2F%2Fmail.google.com%2Fmail%2F%3Ftab%3Dwm&amp;service=mail&amp;rm=false&amp;ltmpl=default&amp;scc=1&amp;osid=1&amp;emr=1\">\n",
      "  Sign in with a different account\n",
      "  </a>\n",
      "False\n",
      "<a href=\"https://accounts.google.com/SignUp?service=mail&amp;continue=https%3A%2F%2Fmail.google.com%2Fmail%2F%3Ftab%3Dwm&amp;ltmpl=default\">\n",
      "  Create account\n",
      "  </a>\n",
      "False\n",
      "<a href=\"https://www.google.com/intl/en/about\" target=\"_blank\">\n",
      "  About Google\n",
      "  </a>\n",
      "False\n",
      "<a href=\"https://accounts.google.com/TOS?loc=US&amp;hl=en&amp;privacy=true\" target=\"_blank\">\n",
      "  Privacy\n",
      "  </a>\n",
      "False\n",
      "<a href=\"https://accounts.google.com/TOS?loc=US&amp;hl=en\" target=\"_blank\">\n",
      "  Terms\n",
      "  </a>\n",
      "False\n",
      "<a href=\"http://www.google.com/support/accounts?hl=en\" target=\"_blank\">\n",
      "  Help\n",
      "  </a>\n",
      "False\n",
      "https://accounts.google.com/ServiceLogin?service=mail&passive=true&rm=false&continue=https://mail.google.com/mail/?tab%3Dwm&scc=1&ltmpl=default&ltmplcache=2&emr=1&osid=1\n"
     ]
    }
   ],
   "source": [
    "# www.google.com\n",
    "\n",
    "# TODO www.gmail.com is  not the same site as opened in the browser\n",
    "\n",
    "mail = \"mail\"\n",
    "signin = \"Sign in\"\n",
    "signup = \"Create account\"\n",
    "policy = \"Use 8 or more\"\n",
    "url = \"https://www.google.com\"\n",
    "\n",
    "r = requests.get(url)\n",
    "c = r.content\n",
    "\n",
    "soup = BeautifulSoup(c, \"html.parser\")\n",
    "text_content = str(c)\n",
    "\n",
    "\n",
    "#signup_str = soup.find(string = re.compile(signup))\n",
    "#print(signup_str)\n",
    "#print(soup.prettify())\n",
    "for link in soup.find_all(\"a\"):\n",
    "    if mail in link.get(\"href\"):\n",
    "        url = link.get(\"href\")\n",
    "        break\n",
    "\n",
    "print(url)\n",
    "\n",
    "\n",
    "r = requests.get(url)\n",
    "c = r.content\n",
    "\n",
    "data = requests.request(\"GET\", url)\n",
    "url = data.url\n",
    "\n",
    "print(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(c, \"html.parser\")\n",
    "text_content = str(c)\n",
    "\n",
    "# at www.gmail.com\n",
    "\n",
    "for link in soup.find_all(\"a\"):\n",
    "    #print(link)\n",
    "    print(signup in link)\n",
    "    if signup in link:\n",
    "        url = link.get(\"href\")\n",
    "        break\n",
    "        \n",
    "print(url)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Use 8 or more characters with a mix of letters, numbers & symbols']\n"
     ]
    }
   ],
   "source": [
    "# Scrape directly from gmail create account page\n",
    "\n",
    "url = \"https://accounts.google.com/signup/v2/webcreateaccount?service=mail&continue=https%3A%2F%2Fmail.google.com%2Fmail%2F%3Fpc%3Dtopnav-about-en&flowName=GlifWebSignIn&flowEntry=SignUp\"\n",
    "\n",
    "\n",
    "r = requests.get(url)\n",
    "c = r.content\n",
    "\n",
    "soup = BeautifulSoup(c, \"html.parser\")\n",
    "\n",
    "s = soup.find_all(string = re.compile(policy))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://login.yahoo.com/account/create?src=fpctx&intl=us&lang=en-US&done=https%3A%2F%2Fwww.yahoo.com&specId=yidReg\n",
      "Please use only letters, numbers, and common punctuation characters\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# www.yahoo.com\n",
    "# ------------------WORKS--------------------------- #\n",
    "\n",
    "\n",
    "url = \"https://www.yahoo.com\"\n",
    "policy = \"Please use only\"\n",
    "\n",
    "signup = \"Sign up\"\n",
    "signup2 = \"Sign&nbsp;up\"\n",
    "signup3 = \"Sign&nbsp; up\"\n",
    "signup4 = \"Sign\"\n",
    "signup5 = \"account\"\n",
    "\n",
    "signin = \"Sign in\"\n",
    "\n",
    "r = requests.get(url)\n",
    "c = r.content\n",
    "\n",
    "soup = BeautifulSoup(c, \"html.parser\")\n",
    "\n",
    "# First check the presence of policy on the homepage\n",
    "# If not available, check the presence of a sign up link\n",
    "# If sign up not available, check sign in link\n",
    "\n",
    "if len(soup.find_all(string = re.compile(policy))) == 0:\n",
    "    # Check sign up link\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        if signup in link:\n",
    "            print(\"sign up present\")\n",
    "            break\n",
    "    \n",
    "    # Check sign in link\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        if signin in link:\n",
    "            url = link.get(\"href\")\n",
    "            break\n",
    "    \n",
    "    #\n",
    "    # Go to sign in page and look for sign up link\n",
    "    #\n",
    "    r = requests.get(url)\n",
    "    c = r.content\n",
    "\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "\n",
    "    # Get the current URL and the netlocal\n",
    "    data = requests.request(\"GET\", url)\n",
    "    url = data.url\n",
    "    \n",
    "    netlocal = urlparse(url).netloc\n",
    "    \n",
    "    for link in soup.find_all(\"a\"):\n",
    "        if signup in link:\n",
    "            print(\"hello\")\n",
    "        elif signup5 in link.get(\"href\"):\n",
    "            #print(link.get(\"href\"))\n",
    "            url = \"https://\" + netlocal + link.get(\"href\")\n",
    "            print(url)\n",
    "            break\n",
    "    \n",
    "    #\n",
    "    # Go to the signup page and look for policy\n",
    "    #\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    c = r.content\n",
    "    \n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    \n",
    "    policy_str = soup.find(string = re.compile(policy))\n",
    "    #print(policy_str)\n",
    "    \n",
    "    index = policy_str.find(policy)\n",
    "    res_policy = \"\"\n",
    "    \n",
    "    while(policy_str[index] != \".\"):\n",
    "        res_policy = res_policy + policy_str[index]\n",
    "        index = index+1\n",
    "    \n",
    "    print(res_policy)\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.instagram.com\n",
    "# ---------------------- NO POLICY!!! ----------------------- #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
